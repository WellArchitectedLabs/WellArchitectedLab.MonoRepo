apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: grafana
  namespace: argocd
  labels:
    workload: operations
    isGrafana: "true"
  annotations:
    argocd.argoproj.io/sync-wave: "16" # Install grafana at the last step after all dependencies are installed
spec:
  project: default
  source:
    repoURL: https://grafana.github.io/helm-charts
    chart: grafana
    targetRevision: 7.3.9
    helm:
      values: |
        # Only one replica is sufficient for most production ready setups
        replicas: 1

        # Secure grafana admin credentials via existing kubernetes secret
        admin:
          existingSecret: grafana-secrets
          userKey: grafana-admin-user
          passwordKey: grafana-admin-password

        # Tolerate the grafana instance to run into the weather forecast node
        # Ideally, a separate node pool for monitoring (prometheus, grafana) and operations
        # Should be used.
        # We are hitting a quotas limitation so we are using only one node pool
        # tolerations:
        #   - key: "workload"
        #     operator: "Equal"
        #     value: "weather-forecast"
        #     effect: "NoSchedule"
        nodeSelector:
            kubernetes.azure.com/mode: system

        # The datasource simply points to the prometheus service 
        # that is deployed via the prometheus argocd app
        # the kubernetes datasource points to current cluster
        datasources:
          datasources.yaml:
            apiVersion: 1
            datasources:

              # local prometheus service
              - name: Prometheus
                type: prometheus
                access: proxy
                url: http://prometheus-server-kube-pro-prometheus.monitoring.svc.cluster.local:9090
                isDefault: true
                editable: false
                uid: prometheus-uid

              - name: Tempo
                type: tempo
                access: proxy
                url: http://tempo.monitoring.svc.cluster.local:3100
                uid: tempo-uid
                jsonData:
                  # # necessary section for mapping tempo traces to metrics
                  # # this will allow advanced grafana features like metrics jump to metrics from a trace for more context on the service health when trace appeared
                  # tracesToMetrics:
                  #   datasourceUid: prometheus-uid
                  #   spanStartTimeShift: -1h
                  #   spanEndTimeShift: 1h
                  #   tags:
                  #   - key: service.name
                  serviceMap:
                    # map the tempo data source to the graph data source using the datasource uid
                    # below is the graph data source configuration
                    datasourceUid: prometheus-service-graph
                  nodeGraph:
                    enabled: true
                  search:
                    hide: false
                  # metricsQueries:
                  #   # just a test query for debugging 
                  #   - name: "Sanity"
                  #     query: up
                  # Trace to logs correlation
                  tracesToLogs:
                    datasourceUid: loki-uid
                    tags:
                      - service
                      - pod
                      - namespace
                      - container
                    mappedTags:
                      - key: service.name
                        value: service
                    spanStartTimeShift: -1h
                    spanEndTimeShift: 1h
                    filterByTraceID: true
                    filterBySpanID: false
                editable: false

              # dedicated datasource for service graph
              # uses the prometheus server url as a backend
              # please note that it is intended to completely separate this data source from the "normal" prometheus datasource above.
              - name: Prometheus-ServiceGraph
                type: prometheus
                access: proxy
                url: http://prometheus-server-kube-pro-prometheus.monitoring.svc.cluster.local:9090
                uid: prometheus-service-graph
                editable: false

              - name: Loki
                type: loki
                access: proxy
                url: http://loki-gateway.monitoring.svc.cluster.local
                uid: loki-uid
                jsonData:
                  maxLines: 1000
                  derivedFields:
                    - datasourceUid: tempo-uid
                      matcherRegex: 'trace_id=(\w+)'
                      name: TraceIDRegex1
                      url: $${__value.raw}

                    - datasourceUid: tempo-uid
                      matcherRegex: 'traceID[":]\s*["'']?(\w+)'
                      name: TraceIDRegex2
                      url: $${__value.raw}

                    - datasourceUid: tempo-uid
                      matcherRegex: 'trace[_-]?id[":]\s*["'']?(\w+)'
                      name: TraceIDRegex3
                      url: $${__value.raw}
                editable: false

        
        # Use the deployed postgres instance to store grafana data

        # Load all secret keys as environment variables
        envFromSecret: grafana-secrets
        
        # Reference the environment variables for database configuration
        env:
          GF_DATABASE_TYPE: postgres
          GF_DATABASE_HOST: grafana-database-postgresql.monitoring.svc.cluster.local:5432
          GF_DATABASE_NAME: $__env{GF_POSTGRES_DB}
          GF_DATABASE_USER: $__env{GF_POSTGRES_USER}
          GF_DATABASE_PASSWORD: $__env{GF_POSTGRES_PASSWORD}
          GF_DATABASE_SSL_MODE: disable

        # It is important that grafana database is persisted
        # Otherwise, dashboards and data will be reset on cluster restart or any event happening in the nodes
        persistence:
          type: pvc
          enabled: true
          storageClassName: managed-csi
          # Makes sure mvcs are deployed with the right access modes
          accessModes:
            - ReadWriteOnce
          size: 10Gi

  destination:
    server: https://kubernetes.default.svc
    namespace: monitoring
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
      - CreateNamespace=true
    retry:
      limit: 5
      backoff:
        duration: 5s
        factor: 2
        maxDuration: 3m