apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: grafana
  namespace: argocd
  labels:
    workload: operations
    isGrafana: "true"
  annotations:
    argocd.argoproj.io/sync-wave: "14" # We need the grafana server installed after database, otlp collector and Tempo tracing db
spec:
  project: default
  source:
    repoURL: https://grafana.github.io/helm-charts
    chart: grafana
    targetRevision: 7.3.9
    helm:
      values: |
        # Only one replica is sufficient for most production ready setups
        replicas: 1

        # Secure grafana admin credentials via existing kubernetes secret
        admin:
          existingSecret: grafana-secrets
          userKey: grafana-admin-user
          passwordKey: grafana-admin-password

        # Tolerate the grafana instance to run into the weather forecast node
        # Ideally, a separate node pool for monitoring (prometheus, grafana) and operations
        # Should be used.
        # We are hitting a quotas limitation so we are using only one node pool
        tolerations:
          - key: "workload"
            operator: "Equal"
            value: "weather-forecast"
            effect: "NoSchedule"

        # Select user nodes
        nodeSelector:
          kubernetes.azure.com/mode: user

        # The datasource simply points to the prometheus service 
        # that is deployed via the prometheus argocd app
        # the kubernetes datasource points to current cluster
        datasources:
          datasources.yaml:
            apiVersion: 1
            datasources:
              # local prometheus service
              - name: Prometheus
                type: prometheus
                access: proxy
                url: http://prometheus-server-kube-pro-prometheus.monitoring.svc.cluster.local:9090
                isDefault: true
                editable: false
              # current kubernetes cluster
              - name: Kubernetes
                type: kubernetes
                access: proxy
                url: https://kubernetes.default.svc
                jsonData:
                  authType: serviceAccount
                  tlsSkipVerify: true
                editable: false
              - name: Tempo
                type: tempo
                access: proxy
                url: http://tempo.monitoring.svc.cluster.local:3100
                jsonData:
                  tracesToMetrics:
                    datasourceUid: Prometheus
                    spanStartTimeShift: "-1h"
                    spanEndTimeShift: "1h"
                    tags: ["service.name", "http.method"]
                editable: false
        
        # Use the deployed postgres instance to store grafana data

        # Load all secret keys as environment variables
        envFromSecret: grafana-secrets
        
        # Reference the environment variables for database configuration
        env:
          GF_DATABASE_TYPE: postgres
          GF_DATABASE_HOST: grafana-database-postgresql.monitoring.svc.cluster.local:5432
          GF_DATABASE_NAME: $__env{GF_POSTGRES_DB}
          GF_DATABASE_USER: $__env{GF_POSTGRES_USER}
          GF_DATABASE_PASSWORD: $__env{GF_POSTGRES_PASSWORD}
          GF_DATABASE_SSL_MODE: disable
        
        # It is important that grafana database is persisted
        # Otherwise, dashboards and data will be reset on cluster restart or any event happening in the nodes
        persistence:
          type: pvc
          enabled: true
          storageClassName: managed-csi
          # Makes sure mvcs are deployed with the right access modes
          accessModes:
            - ReadWriteOnce
          size: 10Gi

  destination:
    server: https://kubernetes.default.svc
    namespace: monitoring
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
      - CreateNamespace=true
    retry:
      limit: 5
      backoff:
        duration: 5s
        factor: 2
        maxDuration: 3m